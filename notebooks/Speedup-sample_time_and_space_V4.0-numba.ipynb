{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Strategie - STEP 4\n",
    "* include NumbaÂ´s Just-In-Time-Compiler into optimization\n",
    "  - enable numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theano version:  1.0.4\n",
      "Numba version:  0.50.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Theano version: \",theano.__version__)\n",
    "print(\"Numba version: \",numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Read/Create Input Data\n",
    "* output:\n",
    "  - kw_data\n",
    "  - day_data\n",
    "  - time_by_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../data/counties/counties.pkl', \"rb\") as f:\n",
    "    counties = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data_complete/diseases/covid19.csv' does not exist: b'../data_complete/diseases/covid19.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-63bab1dc7016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mindata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_daily_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisease\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_region\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-63bab1dc7016>\u001b[0m in \u001b[0;36mload_daily_data\u001b[1;34m(disease, prediction_region, counties, seperator)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_daily_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisease\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_region\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseperator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     data = pd.read_csv(\"../data_complete/diseases/{}.csv\".format(disease),\n\u001b[1;32m----> 5\u001b[1;33m                        sep=seperator, encoding='iso-8859-1', index_col=0)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"99999\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data_complete/diseases/covid19.csv' does not exist: b'../data_complete/diseases/covid19.csv'"
     ]
    }
   ],
   "source": [
    "disease = \"covid19\"\n",
    "prediction_region = \"germany\"\n",
    "def load_daily_data(disease, prediction_region, counties, seperator=\",\"):\n",
    "    data = pd.read_csv(\"../data_complete/diseases/{}.csv\".format(disease),\n",
    "                       sep=seperator, encoding='iso-8859-1', index_col=0)\n",
    "\n",
    "    if \"99999\" in data.columns:\n",
    "        data.drop(\"99999\", inplace=True, axis=1)\n",
    "\n",
    "    data = data.loc[:, list(\n",
    "        filter(lambda cid: prediction_region in counties[cid][\"region\"], data.columns))]\n",
    "    data.index = [pd.Timestamp(date) for date in data.index]\n",
    "\n",
    "    return data\n",
    "indata = load_daily_data(disease, prediction_region, counties)\n",
    "data = indata\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create times_by_day dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "rnd_tsel = np.random.Generator(np.random.PCG64(12345))\n",
    "\n",
    "def uniform_times_by_day(days, n=10):\n",
    "    \"\"\" Samples n random timepoints within a day, per day. converts pd.Timestamps to datetime obj.\"\"\"\n",
    "    res = OrderedDict()\n",
    "    for day in days:\n",
    "        time_min = datetime.datetime.combine(day, datetime.time.min)\n",
    "        time_max = datetime.datetime.combine(day, datetime.time.max)\n",
    "        res[day] = rnd_tsel.random(n) * (time_max - time_min) + time_min\n",
    "    return res\n",
    "\n",
    "times_by_day=uniform_times_by_day(data.index)\n",
    "#times_by_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create locations_by_county dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "rnd_csel = np.random.Generator(np.random.PCG64(12345))\n",
    "\n",
    "def uniform_locations_by_county(counties, n=5):\n",
    "    res = OrderedDict()\n",
    "    for (county_id, county) in counties.items():\n",
    "        tp = county[\"testpoints\"]\n",
    "        if n == len(tp):\n",
    "            res[county_id] = tp\n",
    "        else:\n",
    "            idx = rnd_csel.choice(tp.shape[0], n, replace=n > len(tp))\n",
    "            res[county_id] = tp[idx]\n",
    "    return res\n",
    "\n",
    "locations_by_county=uniform_locations_by_county(counties)\n",
    "#locations_by_county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define temporal_bfs and spatial_bfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_bf(dx, Ï):\n",
    "    \"\"\" spatial basis function \"\"\"\n",
    "    Ï = np.float32(Ï)\n",
    "    res = tt.zeros_like(dx)\n",
    "    idx = (abs(dx) < np.float32(5) * Ï)  # .nonzero()\n",
    "    return tt.set_subtensor(res[idx], tt.exp(\n",
    "        np.float32(-0.5 / (Ï**2)) * (dx[idx])**2) / np.float32(np.sqrt(2 * np.pi * Ï**2)))\n",
    "\n",
    "\n",
    "def bspline_bfs(x, knots, P):\n",
    "    \"\"\" temporal basis function\n",
    "            x: t-delta distance to last knot (horizon 5)\n",
    "    \"\"\"\n",
    "    knots = knots.astype(np.float32)\n",
    "    idx = ((x >= knots[0]) & (x < knots[-1]))  # .nonzero()\n",
    "    xx = x[idx]\n",
    "\n",
    "    N = {}\n",
    "    for p in range(P + 1):\n",
    "        for i in range(len(knots) - 1 - p):\n",
    "            if p == 0:\n",
    "                N[(i, p)] = tt.where((knots[i] <= xx)\n",
    "                                     * (xx < knots[i + 1]), 1.0, 0.0)\n",
    "            else:\n",
    "                N[(i, p)] = (xx - knots[i]) / (knots[i + p] - knots[i]) * N[(i, p - 1)] + \\\n",
    "                    (knots[i + p + 1] - xx) / (knots[i + p + 1] - knots[i + 1]) * N[(i + 1, p - 1)]\n",
    "\n",
    "    highest_level = []\n",
    "    for i in range(len(knots) - 1 - P):\n",
    "        res = tt.zeros_like(x)\n",
    "        highest_level.append(tt.set_subtensor(res[idx], N[(i, P)]))\n",
    "    return highest_level\n",
    "\n",
    "\n",
    "#NOTE: Do we want basis functions with a longer temporal horizon? // we may want to weight them around fixed days?!\n",
    "#NOTE: Split this up, so we can get multiple basis functions!\n",
    "def temporal_bfs(x):\n",
    "    return bspline_bfs(x, np.array([0, 0, 1, 2, 3, 4, 5]) * 24 * 3600.0, 2) \n",
    "\n",
    "\n",
    "def spatial_bfs(x):\n",
    "    return [gaussian_bf(x, Ï) for Ï in [6.25, 12.5, 25.0, 50.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Theano function ia_bfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_sq(latitude, R=6365.902):\n",
    "    \"\"\"\n",
    "        jacobian_sq(latitude)\n",
    "\n",
    "    Computes the \"square root\" (Cholesky factor) of the Jacobian of the cartesian projection from polar coordinates (in degrees longitude, latitude) onto cartesian coordinates (in km east/west, north/south) at a given latitude (the projection's Jacobian is invariante wrt. longitude).\n",
    "    TODO: don't import jacobian_sq from geo_utils to remove potential conflicts\n",
    "    \"\"\"\n",
    "    return R * (np.pi / 180.0) * (abs(tt.cos(tt.deg2rad(latitude))) *\n",
    "                                  np.array([[1.0, 0.0], [0.0, 0.0]]) + np.array([[0.0, 0.0], [0.0, 1.0]]))\n",
    "\n",
    "\n",
    "def build_ia_bfs(temporal_bfs, spatial_bfs, profile):\n",
    "    x1 = tt.fmatrix(\"x1\")\n",
    "    t1 = tt.fvector(\"t1\")\n",
    "    # M = tt.fmatrix(\"M\")\n",
    "    x2 = tt.fmatrix(\"x2\")\n",
    "    t2 = tt.fvector(\"t2\")\n",
    "\n",
    "    lat = x1[:, 1].mean()\n",
    "    M = jacobian_sq(lat)**2\n",
    "\n",
    "    # (x1,t1) are the to-be-predicted points, (x2,t2) the historic cases\n",
    "\n",
    "    # spatial distance btw. each points (defined with latitude,longitude) in x1 and x2 with gramian M\n",
    "    # (a-b)^2 = a^2 + b^2 -2ab; with a,b=vectors\n",
    "    dx = tt.sqrt(  (x1.dot(M) * x1).sum(axis=1).reshape((-1,  1)) # a^2\n",
    "                 + (x2.dot(M) * x2).sum(axis=1).reshape(( 1, -1)) # b^2\n",
    "                 - 2 * x1.dot(M).dot(x2.T) )                      # -2ab\n",
    "\n",
    "    # temporal distance btw. each times in t1 and t2\n",
    "    dt = t1.reshape((-1, 1)) - t2.reshape((1, -1))\n",
    "\n",
    "    ft = tt.stack(temporal_bfs(dt.reshape((-1,))), axis=0) # cast to floats?\n",
    "    fx = tt.stack(spatial_bfs(dx.reshape((-1,))), axis=0)\n",
    "\n",
    "    # aggregate contributions of all cases\n",
    "    contrib = ft.dot(fx.T).reshape((-1,)) / tt.cast(x1.shape[0], \"float32\")\n",
    "\n",
    "    return theano.function([t1, x1, t2, x2], contrib, allow_input_downcast=True, profile=profile)\n",
    "\n",
    "ia_bfs = build_ia_bfs(temporal_bfs, spatial_bfs, profile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_ia_bfs = build_ia_bfs(temporal_bfs, spatial_bfs, profile=True)\n",
    "theano.printing.debugprint(profile_ia_bfs)\n",
    "\n",
    "# test ia_bfs()\n",
    "t1=[1580234892.375513, 1580224126.122202, 1580193367.920551, 1580193367.920551, 1580185641.832341, 1580194755.123367]\n",
    "x1 = [ [10.435944369180099, 51.69958916804793],\n",
    "       [10.435944369180099, 51.69958916804793],\n",
    "       [10.134378974970323, 51.51153765399198],\n",
    "       [10.134378974970323, 51.51153765399198],\n",
    "       [10.435944369180099, 51.69958916804793],\n",
    "       [10.97023632180951,  49.35209111265112],]\n",
    "t2=[1580234892.375513, 1580224428.403552, 1580182133.833636, 1580217693.876309, 1580224428.403552, 1580224428.403552,]\n",
    "x2 = [ [11.38965623, 48.0657035 ],\n",
    "       [11.0615104 , 48.11177134],\n",
    "       [ 7.12902758, 51.57865701],\n",
    "       [ 7.12902758, 51.57865701],\n",
    "       [11.38965623, 48.0657035 ],\n",
    "       [11.0615104 , 48.11177134],]\n",
    "profile_ia_bfs(t1,x1,t2,x2)\n",
    "profile_ia_bfs.profile.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Do the Sampling (the old way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to check results\n",
    "rnd_time = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_loc  = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_time_pred = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_loc_pred  = np.random.Generator(np.random.PCG64(12345))\n",
    "\n",
    "# random generators:\n",
    "# MT19937, PCG64, Philox, SFC64 - https://numpy.org/devdocs/reference/random/bit_generators/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# loop over all days of all counties\n",
    "# and draw per day n-times a random time from times_by_day[day]\n",
    "\n",
    "def sample_time_and_space(data, times_by_day, locations_by_county, rnd_t, rnd_l):\n",
    "    n_total = data.sum().sum()\n",
    "    t_all = np.empty((n_total,), dtype=object)\n",
    "    x_all = np.empty((n_total, 2))\n",
    "    \n",
    "    i=0\n",
    "    for (county_id, series) in data.iteritems():\n",
    "        for (day, n) in series.iteritems():\n",
    "            #if n==0: continue\n",
    "            #print(i,\"\\n   day =\",day,\"\\n   no. samples to draw = \",n)\n",
    "        \n",
    "            # draw n random times\n",
    "            times = times_by_day[day]\n",
    "            #idx = rnd_time.choice(len(times), n)\n",
    "            idx = np.floor( (n*[len(times)]) * rnd_t.random((n,)) ).astype(\"int32\") # replace 'rnd_time.choice' to enable compare with new optimized solution\n",
    "            #print(\"   random sample ids   = \",idx)\n",
    "            t_all[i:i + n] = times[idx]\n",
    "\n",
    "            # draw n random locations\n",
    "            locs = locations_by_county[county_id]\n",
    "            #idx = rnd_loc.choice(locs.shape[0], n)\n",
    "            idx = np.floor( (n*[locs.shape[0]]) * rnd_l.random((n,)) ).astype(\"int32\") # replace 'rnd_time.choice' to enable compare with new optimized solution\n",
    "            x_all[i:i + n, :] = locs[idx, :]\n",
    "        \n",
    "            i += n          \n",
    "\n",
    "    return t_all, x_all\n",
    "\n",
    "t_data_0 = []\n",
    "x_data_0 = []\n",
    "t_pred_0 = []\n",
    "x_pred_0 = []\n",
    "\n",
    "num_tps=5\n",
    "d_offs=0 # just to limit the time of test\n",
    "c_offs=0 # just to limit the time of test\n",
    "days = data.index[d_offs:d_offs+50]\n",
    "counties = data.columns[c_offs:c_offs+50]\n",
    "\n",
    "_to_timestamp = np.frompyfunc(datetime.datetime.timestamp, 1, 1)\n",
    "num_features = len(temporal_bfs(tt.fmatrix(\"tmp\"))) * len(spatial_bfs(tt.fmatrix(\"tmp\")))\n",
    "res_0 = np.zeros((len(days), len(counties), num_features), dtype=np.float32)\n",
    "\n",
    "for i, day in enumerate(days):\n",
    "    for j, county in enumerate(counties):\n",
    "        idx = ((day - pd.Timedelta(days=5)) <= data.index) * (data.index < day)\n",
    "\n",
    "        t_data, x_data = sample_time_and_space(data.iloc[idx], times_by_day, locations_by_county, rnd_time, rnd_loc)\n",
    "        t_pred, x_pred = sample_time_and_space(pd.DataFrame(num_tps, index=[day], columns=[county]), times_by_day, locations_by_county, rnd_time_pred, rnd_loc_pred)\n",
    "        \n",
    "        #print(\"_to_timestamp(t_pred) (types, type1, size, value): \", type(_to_timestamp(t_pred)), type(_to_timestamp(t_pred)[0]), np.shape(_to_timestamp(t_pred)), _to_timestamp(t_pred)[0])\n",
    "        # => _to_timestamp(t_pred) (types, type1, size, value):  <class 'numpy.ndarray'> <class 'float'> (5,) 1580217693.876309\n",
    "        #print(\"x_pred (types, size, value)       : \", type(x_pred), type(x_pred[0]), type(x_pred[0][0]), np.shape(x_pred), x_pred[0][0])        \n",
    "        # => x_pred (types, size, value)       :  <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.float64'> (5, 2) 10.134378974970323\n",
    "        \n",
    "        res_0[i, j, :] = ia_bfs(_to_timestamp(t_pred), x_pred, _to_timestamp(t_data), x_data)        \n",
    "        \n",
    "        # store all to compare with old algo\n",
    "        t_data_0 = t_data_0 + t_data.tolist()\n",
    "        x_data_0 = x_data_0 + x_data.tolist()\n",
    "        t_pred_0 = t_pred_0 + t_pred.tolist()\n",
    "        x_pred_0 = x_pred_0 + x_pred.tolist()\n",
    "\n",
    "######## output ########\n",
    "#display(t_data_0[:2])\n",
    "#display(x_data_0[:2])\n",
    "#display(t_pred_0[:2])\n",
    "#display(x_pred_0[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0[1:2][:][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C - Do the Sampling (the NEW way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## C3 - COMPACT result\n",
    "* requires (A) to be finished -> data, times_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_time_and_space__once(times_by_day, locations_by_county):\n",
    "    \"\"\" \n",
    "    Convert dictonarys to arrays for faster access in sample_time_and_space().\n",
    "  \n",
    "    Random access in times_by_day and locations_by_county are very costy.\n",
    "    Hence they need to be converted to arrays and access must be done through indexes.\n",
    "    \"\"\"\n",
    "    # times_by_day_np[day-id] => times[n_times]\n",
    "    times_by_day_np = pd.DataFrame.from_dict(times_by_day,orient='index').to_numpy(dtype='datetime64') # => type=='numpy.datetime64'\n",
    "    \n",
    "    t_convert_1 = np.frompyfunc(pd.Timestamp, 1, 1)\n",
    "    times_by_day_np = t_convert_1(times_by_day_np) # => type=='pandas._libs.tslibs.timestamps.Timestamp'\n",
    "    \n",
    "    t_convert_2 = np.frompyfunc(datetime.datetime.timestamp, 1, 1)\n",
    "    times_by_day_np = t_convert_2(times_by_day_np) # => type=='float'\n",
    "    times_by_day_np = np.array(times_by_day_np, np.float64) # need to convert this to np.float64 for numba\n",
    "    \n",
    "    # locations_by_county_np[county-id] => locs[m_locs[x,y]]\n",
    "    max_coords = 0\n",
    "    for item in locations_by_county.items():\n",
    "        max_coords = max( len(item[1]), max_coords)\n",
    "    locations_by_county_np = np.empty([len(locations_by_county.keys()), max_coords, 2], dtype='float64')\n",
    "    for i,item in enumerate(locations_by_county.items()): # counties are sorted because of OrderedDict\n",
    "        locations_by_county_np[i][:] = item[1][:]\n",
    "        \n",
    "    return(times_by_day_np, locations_by_county_np)\n",
    "\n",
    "#times_by_day_np, locations_by_county_np = sample_time_and_space__once(times_by_day, locations_by_county)\n",
    "#print(\"locations_by_county_np (types, size, value) : \",\n",
    "#      type(locations_by_county_np),\n",
    "#      type(locations_by_county_np[0]),\n",
    "#      type(locations_by_county_np[0][0]),\n",
    "#      np.shape(locations_by_county_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_time_and_space__prep(times_by_day_np, locations_by_county_np, data, idx):\n",
    "    \"\"\" \n",
    "    Recalculations for a fixed dataframe sample_time_and_space().\n",
    "  \n",
    "    Calculation of helper arrays are very costy.\n",
    "    If the dataframe does not change, precalculated values can be reused.\n",
    "    \"\"\"\n",
    "\n",
    "    # subdata 'data' of 'indata' is likely to skip a few first days(rows) in 'indata',\n",
    "    # but as times_by_day_np represents the whole 'indata', an offsets needs to be considered when accessing 'times_by_day_np'\n",
    "    dayoffset = np.where(idx==True)[0][0]   \n",
    "    n_total = data.sum().sum()\n",
    "\n",
    "    # get number of samples per county-day\n",
    "    smpls_per_cntyday = np.array(data.values).flatten('F')\n",
    "\n",
    "    ######## t_all ########\n",
    "\n",
    "    # get list of day-ids for all county-days\n",
    "    dayids = np.arange(len(data.index))\n",
    "    day_of_cntyday = np.tile(dayids, len(data.columns))\n",
    "\n",
    "    # get list of day-ids for all samples, use numpy.array for speedup in numba\n",
    "    day_of_smpl = np.array([ day_of_cntyday[i] for (i,smpls) in enumerate(smpls_per_cntyday) for x in range(smpls) ])\n",
    "\n",
    "    # get available times for each sample\n",
    "    time_of_days = data.index.tolist() # cannot be a np.array as it needs to stay a pandas.timeformat\n",
    "    av_times_per_day = [len(times_by_day[d]) for d in time_of_days]\n",
    "    av_times_per_smpl = [ av_times_per_day[day_of_cntyday[i]] for (i,smpls) in enumerate(smpls_per_cntyday) for x in range(smpls) ]\n",
    "    \n",
    "    ######## x_all ########\n",
    "\n",
    "    # get list of county-ids for all county-days\n",
    "    cntyids = np.arange(len(data.columns))\n",
    "    cnty_of_cntyday = np.repeat(cntyids, len(data.index))\n",
    "\n",
    "    # get list of county-ids for all samples, use numpy.array for speedup in numba\n",
    "    cnty_of_smpl = np.array([ cnty_of_cntyday[i] for (i,smpl) in enumerate(smpls_per_cntyday) for x in range(smpl) ])\n",
    "\n",
    "    # get available locations for each sample\n",
    "    label_of_cntys = data.columns # list of countys labels\n",
    "    av_locs_per_cnty = [len(locations_by_county[c]) for c in label_of_cntys]\n",
    "    av_locs_per_smpl = [ av_locs_per_cnty[cnty_of_cntyday[i]] for (i,smpls) in enumerate(smpls_per_cntyday) for x in range(smpls) ]\n",
    "    \n",
    "    return (n_total, dayoffset,\n",
    "            day_of_smpl, av_times_per_smpl, \n",
    "            cnty_of_smpl, av_locs_per_smpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_time_and_space__pred(n_days, n_counties, d_offs, c_offs, num_tps, av_times_per_smpl, av_locs_per_smpl, rnd_time, rnd_loc):\n",
    "    \n",
    "    ######## t_all ########    \n",
    "    n_total = n_days * n_counties * num_tps\n",
    "    \n",
    "    rnd_timeid_per_smpl = np.floor( av_times_per_smpl * rnd_time.random( n_total ) ).astype(\"int32\")\n",
    "    \n",
    "    # collect times for each sample with its random time-id\n",
    "    t_all = [ times_by_day_np[d_offs+i][rnd_timeid_per_smpl[(i*n_counties+j)*num_tps+x]] for i in range(n_days) for j in range(n_counties) for x in range(num_tps) ] \n",
    "\n",
    "    ######## x_all ########\n",
    "\n",
    "    # calc random location-id for each sample\n",
    "    rnd_locid_per_smpl = np.floor( av_locs_per_smpl * rnd_loc.random((n_total,)) ).astype(\"int32\")\n",
    "\n",
    "    # collect locations for each sample with its random location-id\n",
    "    x_all = [ locations_by_county_np[c_offs+j][rnd_locid_per_smpl[(i*n_counties+j)*num_tps+x]] for i in range(n_days) for j in range(n_counties) for x in range(num_tps) ] \n",
    "    \n",
    "    return t_all, x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True, parallel=False, cache=False)\n",
    "def sample_time_and_space_tall(n_total, n_counties, dayoffset, day_of_smpl, rnd_timeid_per_smpl_all, times_by_day_np):\n",
    "    # ensure we return a numpy array for better preformance\n",
    "    return np.array([ times_by_day_np[day+dayoffset][rnd_timeid_per_smpl_all[j*n_total+i]] for j in range(n_counties) for (i,day) in enumerate(day_of_smpl) ], dtype=np.float64) # [county][day][smpl]\n",
    "\n",
    "@numba.jit(nopython=True, parallel=False, cache=False)\n",
    "def sample_time_and_space_xall(n_total, n_counties, dayoffset, cnty_of_smpl, rnd_locid_per_smpl_all, locations_by_county_np):\n",
    "    # ensure we return a numpy array for better preformance\n",
    "    return [ locations_by_county_np[cnty][rnd_locid_per_smpl_all[j*n_total+i]] for j in range(n_counties) for (i,cnty) in enumerate(cnty_of_smpl)] # [county][day][smpl]\n",
    "\n",
    "def sample_time_and_space(times_by_day_np, n_counties, n_total, dayoffset, day_of_smpl, av_times_per_smpl, cnty_of_smpl, av_locs_per_smpl, rnd_time, rnd_loc):\n",
    "    \"\"\" \n",
    "    Calculations samples in time and space.\n",
    "  \n",
    "    Calculation a hughe random number array use precalulated results to pick samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_total == 0:\n",
    "        return np.empty((0,), dtype=np.float64), np.empty((0, 2), dtype=np.float64)\n",
    "           \n",
    "    ######## t_all ########\n",
    "    \n",
    "    # calc random time-id for each sample\n",
    "    n_all = n_total * n_counties\n",
    "    \n",
    "    av_times_per_smpl_all = np.tile(av_times_per_smpl, n_counties)\n",
    "    rnd_timeid_per_smpl_all = np.floor( av_times_per_smpl_all * rnd_time.random( (n_all,) ) ).astype(\"int32\")\n",
    "\n",
    "    # collect times for each sample with its random time-id\n",
    "    #t_all = np.empty((n_total,), dtype=object) \n",
    "    #t_all = [ times_by_day_np[day+dayoffset][rnd_timeid_per_smpl_all[j*n_total+i]] for j in range(n_counties) for (i,day) in enumerate(day_of_smpl) ] # [county][day][smpl]\n",
    "    t_all = sample_time_and_space_tall(n_total, n_counties, dayoffset, day_of_smpl, rnd_timeid_per_smpl_all, times_by_day_np)\n",
    "\n",
    "    ######## x_all ########\n",
    "\n",
    "    # calc random location-id for each sample\n",
    "    av_locs_per_smpl_all = np.tile(av_locs_per_smpl, n_counties)\n",
    "    rnd_locid_per_smpl_all = np.floor( av_locs_per_smpl_all * rnd_loc.random( (n_all,) ) ).astype(\"int32\")\n",
    "\n",
    "    # collect locations for each sample with its random location-id\n",
    "    #x_all = np.empty((n_total, 2))\n",
    "    #x_all = [ locations_by_county_np[cnty][rnd_locid_per_smpl_all[j*n_total+i]] for j in range(n_counties) for (i,cnty) in enumerate(cnty_of_smpl)] # [county][day][smpl]\n",
    "    x_all = sample_time_and_space_xall(n_total, n_counties, dayoffset, cnty_of_smpl, rnd_locid_per_smpl_all, locations_by_county_np)\n",
    "    \n",
    "    return t_all, x_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "# set seed to check results\n",
    "# Parallel Random Number Generation - https://docs.scipy.org/doc/numpy/reference/random/parallel.html\n",
    "# Multithreaded Generation - https://docs.scipy.org/doc/numpy/reference/random/multithreading.html\n",
    "rnd_time = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_loc  = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_time_pred = np.random.Generator(np.random.PCG64(12345))\n",
    "rnd_loc_pred  = np.random.Generator(np.random.PCG64(12345))\n",
    "\n",
    "# Convert dictonarys to arrays for faster access in sample_time_and_space().\n",
    "(times_by_day_np, locations_by_county_np,) = sample_time_and_space__once(times_by_day, locations_by_county)\n",
    "\n",
    "t_data_1 = []\n",
    "x_data_1 = []\n",
    "t_pred_1 = []\n",
    "x_pred_1 = []\n",
    "\n",
    "d_offs=0 # just to limit the time of test\n",
    "c_offs=0 # just to limit the time of test\n",
    "days = data.index[d_offs:d_offs+50]\n",
    "counties = data.columns[c_offs:c_offs+50]\n",
    "\n",
    "num_features = len(temporal_bfs(tt.fmatrix(\"tmp\"))) * len(spatial_bfs(tt.fmatrix(\"tmp\")))\n",
    "res_1 = np.zeros((len(days), len(counties), num_features), dtype=np.float32)\n",
    "\n",
    "num_tps=5\n",
    "n_days = len(days)\n",
    "n_counties = len(counties)\n",
    "\n",
    "# create dataframe with 'num_tps' in each cell\n",
    "pred_data = pd.DataFrame(num_tps, index=days, columns=counties)\n",
    "idx = np.empty([len(data.index)], dtype='bool')\n",
    "idx.fill(True)\n",
    "\n",
    "# precalculate pediction values\n",
    "(n_total, dayoffset, day_of_smpl, av_times_per_smpl, cnty_of_smpl, av_locs_per_smpl,) = sample_time_and_space__prep(times_by_day_np, locations_by_county_np, pred_data, idx)\n",
    "(t_pred_all, x_pred_all,) = sample_time_and_space__pred(n_days, n_counties, d_offs, c_offs, num_tps, av_times_per_smpl, av_locs_per_smpl, rnd_time_pred, rnd_loc_pred)\n",
    "\n",
    "for i, day in enumerate(days):\n",
    "    \n",
    "    # calc which sub-table will be selected\n",
    "    idx = ((day - pd.Timedelta(days=5)) <= data.index) * (data.index < day)\n",
    "    subdata = data.iloc[idx]\n",
    "    \n",
    "    if subdata.size != 0:\n",
    "        #print(i, day)\n",
    "        \n",
    "        # Recalculations for a fixed dataframe sample_time_and_space().\n",
    "        (n_total, dayoffset, day_of_smpl, av_times_per_smpl, cnty_of_smpl, av_locs_per_smpl,) = sample_time_and_space__prep(times_by_day_np, locations_by_county_np, subdata, idx)    \n",
    "\n",
    "        # Calculate time and space samples for all counties at once\n",
    "        (t_data_all, x_data_all,) = sample_time_and_space(times_by_day_np, len(counties), n_total, dayoffset, day_of_smpl, av_times_per_smpl, cnty_of_smpl, av_locs_per_smpl, rnd_time, rnd_loc)\n",
    "        \n",
    "        #print(np.shape(x_data_all), type(x_data_all[0][0]))\n",
    "\n",
    "        for j, county in enumerate(counties):\n",
    "\n",
    "            # calcs only for the single DataFrame.cell[day][county]\n",
    "            offs = (i*n_counties+j)*num_tps\n",
    "            t_pred = t_pred_all[offs:offs+num_tps] \n",
    "            x_pred = x_pred_all[offs:offs+num_tps] \n",
    "    \n",
    "            # get subarray for county==j\n",
    "            t_data = t_data_all[j*n_total:(j+1)*n_total] # [county][smpl]\n",
    "            x_data = x_data_all[j*n_total:(j+1)*n_total] # [county][smpl]\n",
    "             \n",
    "            # use theano.function for day==i and county==j                \n",
    "            res_1[i, j, :] = ia_bfs(t_pred, x_pred, t_data, x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_1[1:2][:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(res_0, res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
